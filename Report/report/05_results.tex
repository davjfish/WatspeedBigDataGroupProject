\section{Results}


\subsection{Database Design and Implementation}

After an initial inspection of the data using exploratory tools from Pandas~\cite{pandas}, a SQL schema of five tables was devised and drafted.
The five resulting tables and their descriptions are as follows:

\begin{itemize}
    \item Category - categorical descriptions of the types of calls received (e.g., car accident)
    \item Township - township name and state (e.g., Kings Township, PA)
    \item ResponseUnit - complete list of units responding to emergency calls  (e.g., Station 123, EMS)
    \item ResponseType - the response unit type (e.g., EMS, Traffic, or Fire)
    \item EmergencyCall - this is the primary data table and used to store information about emergency calls received. It has several links to the above tables.
\end{itemize}

Figure~\ref{fig:sqldiagram} presents a visual portrayal of the five above table and the relationships between them.
In addition to the foreign key constraints between tables, indexes were added to each table to improve performance.
Unique constraints were placed on fields in tables were duplication of data entry was not wanted.
For example, we only wanted there to be a single entry for \"back pain\" in the Category table and only a single entry for \"EMS\" in the ResponseType table.
The Township and ResponseUnit tables both had unique together constraints across two columns.
In the case of the former, only a single entry for a combination of township name and state was desired and for the latter,
only a single combination of response unit and station name was desired.
Finally, the complete SQL definitions of the above tables and indices can be found in \hyperref[sec:app1]{Appendix 1}.

The file size of the original CSV downloaded from Kaggle was approximately 123 MB\@.
Following the creation of the database and subsequent ingestion of the data, the file size was reduced to approximately 75 MB\@, including all the indices.
This confirms that our schema has indeed achieved greater storage efficiency than the CSV flat file.

\subsection{Emergency Response Dashboard Application}

The mockup of the Emergency Response Application can be viewed on the internet \href{https://dfishman.pythonanywhere.com/}{here}.
The application has a landing page, a map, a data summary page and an administration page reserved for manually importing data.
All of these pages are very basic and have the sole purpose of demonstrating a proof of concept.

A screenshot of the map, which utilizes Leaflet, can be seen in Figure~\ref{fig:map}.
Since there is a large number of calls in the database (>600,000), it would take too long to load this in a single call.
This will become even more pronounced overtime as additional data is accumulated.
Therefore, using a paginated API endpoint is of paramount importance---allowing users to load data incrementally.
In our application, the users can decide on how many records to load at a time
(up to a maximum page size of 10,000, see pagination class
~\href{https://github.com/davjfish/WatspeedBigDataGroupProject/blob/main/application/dashboard/paginators.py}{here}).
Additionally, users can hone in on specific data by using filters which are then passed to the API endpoint as query parameters.
For example, a GET request to
~\href{https://dfishman.pythonanywhere.com//api/emergency-calls/?page_size=50&datetime__gte=2015-12-10&datetime__lte=2016-12-10&response_unit=27}{this endpoint}
would return a paginated JSON response of calls received between December 10, 2015 and December 10, 2016, and responded to by EMS Station 313A\@.

In a similar vein, the dashboard models how a stakeholder can explore data summaries and statistics in real time.
To achieve this, we used Charts.js in conjunction with a custom-paired API endpoint.
A screenshot of this page is presented in Figure~\ref{fig:chart}.
The main difference between this view and the map is that the API is calling aggregation functions instead of returning individual call records.
By doing so, user are able to aggregate vast quantities of data and leverage the table indices as well as the powerful SQL engine.
As noted above, these endpoint are going to be custom-paired with specific summaries or analysis.
In the example shown here,~\href{https://dfishman.pythonanywhere.com//api/emergency-calls/?chart1=true}{this endpoint}
returns a response that is highly tailored to the Charts.js bar chart being used on this page.
The response is generated by first applying any filters to the queryset and then grouping the results by category and call count.
Next, the data is sorted into two lists, one corresponding to category names and the other to the respective call counts.
These step can be viewed in our application's code \href{https://github.com/davjfish/WatspeedBigDataGroupProject/blob/main/application/dashboard/views.py#L64}{here}.


\subsection{Importing CSV Data}

One of the challenges we will face in this endeavor is how to load information coming in from disparate data sources.
We will address this by creating a Python class for handling, parsing and ingesting serialized emergency call data.
A separate Parser class will have to be developed for each type of incoming data that is anticipated.
These parsers will be written as Python classes and stored in a file called
\href{https://github.com/davjfish/WatspeedBigDataGroupProject/blob/main/application/dashboard/parsers.py}{parsers.py}

The parser class called \texttt{PA911CSVParser} is what we developed to parse and ingest data that is structured like that found in the Pennsylvania dataset.
The class has one obligatory initial argument called \texttt{file}, which is a BytesIO object held in memory.
when the \texttt{parse()} method is called, the import sequence is then triggered.
First, the data is read into a Pandas DataFrame and cleaned using the
\href{https://github.com/davjfish/WatspeedBigDataGroupProject/blob/main/application/dashboard/parsers.py#L17}{following steps}:

\begin{enumerate}
    \item{
        The response unit type (i.e., EMS, Traffic or Fire) is extracted from the title and stored in a separate column.
        This is strait-forward because the response unit type is always prefixed in the record's title followed by a colon.
    }
    \item {
        The incident category is extracted from the title and stored in a separate column.
        This step is also pretty simple since the category of incident always proceeds the type.
    }
    \item {
        If possible, find a station name in the record's description.
        This is a more complicated step that involves the use of RegEx to detect the different ways in which the station name is provided.
        An additional challenge for this step is that sometimes the description contained an address with a street name called \"Station Rd\".
    }
\end{enumerate}

Next, we iterate through a unique list of all the townships in the data.
Since we are not expecting there to be too many townships, it is safe to proceed item by item in the unique array.
For each item, we strip trailing whitespace from the strings and convert them to uppercase in order to minimize the potential for duplicate entries.
Then, we initialize a \texttt{Township} object and append it to a python list.
At the end of that loop, we execute a bulk create query to add all the townships in a single transaction.
Since there is a unique constraint on a Township's name and state, duplicate entries will be rejected.
However, if we use the \texttt{INSERT or IGNORE} statement to the bulk create, the transaction will still be accepted.

%
%
%
%
%
%
%Specifically, in addition to HTML Django templates, the reactive front-end of the application
%will be  developed using Vue.js~\cite{vue}, Leaflet~\cite{leaflet} and Charts.js~\cite{charts}.
%
%
%On the backend of the application a parser class was defined in order to handle the incoming CSV and its digestion into the database.
%- leveraging bulk creates
%
%The Django web-application will also be used to define a Parser class that will be used to ingest the CSV data.
%Th




\pagebreak



\begin{figure}
    \includegraphics[width=\linewidth]{SQL.png}
    \caption{A visual depiction of the five tables in our SQL database design.}
    \label{fig:sqldiagram}
\end{figure}



\begin{figure}
    \includegraphics[width=\linewidth]{map.png}
    \caption{The Emergency Dashboard application's map.}
    \label{fig:map}
\end{figure}


\begin{figure}
    \includegraphics[width=\linewidth]{chart.png}
    \caption{The Emergency Dashboard application's charts page.}
    \label{fig:chart}
\end{figure}

